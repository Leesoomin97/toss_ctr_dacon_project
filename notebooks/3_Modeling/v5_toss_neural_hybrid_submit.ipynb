{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f40d4ec0-8188-4457-bccd-12e0ead97b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import math\n",
    "import random\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import average_precision_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "75751d30-d5ff-412d-b9df-33350e9ca49f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ----------------------\n",
    "# Config\n",
    "# ----------------------\n",
    "CFG = {\n",
    "    'TRAIN_PATH': './train.parquet',           \n",
    "    'TEST_PATH':  './test.parquet',           \n",
    "    'SUBMIT_PATH': './toss_v5_nn_submit.csv',\n",
    "    # Optional meta predictions from v4 (if present, will be stacked)\n",
    "    'V4_XGB_SUB': './toss_xgb_v3_submit.csv',\n",
    "    'V4_LGB_SUB': './toss_lgbm_v2_submit.csv',\n",
    "\n",
    "    'CAT_COLS': [ 'gender', 'age_group', 'inventory_id', 'l_feat_14', 'hour', 'day_of_week' ],\n",
    "    # Numeric columns will be autodetected as (all except target, seq, ID, CAT_COLS)\n",
    "\n",
    "    'SEQ_COL': 'seq',\n",
    "    'TARGET_COL': 'clicked',\n",
    "    'ID_COL': 'ID',\n",
    "\n",
    "    'MAX_SEQ_LEN': 180,        # truncate or pad sequences to this length\n",
    "    'BATCH_SIZE': 1024,\n",
    "    'EPOCHS': 7,\n",
    "    'LR': 1e-3,\n",
    "    'WEIGHT_DECAY': 1e-5,\n",
    "    'SEED': 42,\n",
    "\n",
    "    'EMB_DIM': 24,             # 16~32 reasonable\n",
    "    'RNN_TYPE': 'gru',         # 'gru' or 'lstm'\n",
    "    'RNN_HIDDEN': 96,          # 64~128\n",
    "    'RNN_LAYERS': 2,\n",
    "    'RNN_BIDIR': True,\n",
    "\n",
    "    'CROSS_LAYERS': 2,         # 2~3\n",
    "    'MLP_UNITS': [512, 256, 128],\n",
    "    'DROPOUTS': [0.10, 0.15, 0.20],\n",
    "\n",
    "    'VAL_SIZE': 0.10,          # 10% hold-out validation\n",
    "    'NUM_WORKERS': 2,          # dataloader workers (increase if IO fast)\n",
    "    'PIN_MEMORY': True,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0be9b2f4-4e76-44ea-8ce6-149b0c34b00a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Device: cpu\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------\n",
    "# Utils\n",
    "# ----------------------\n",
    "\n",
    "def seed_everything(seed: int = 42):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "seed_everything(CFG['SEED'])\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"[info] Device: {device}\")\n",
    "\n",
    "# Competition metrics\n",
    "\n",
    "def weighted_logloss(y_true, y_pred, eps=1e-15):\n",
    "    y_pred = np.clip(y_pred, eps, 1 - eps)\n",
    "    mask0 = (y_true == 0)\n",
    "    mask1 = (y_true == 1)\n",
    "    ll0 = -np.mean(np.log(1 - y_pred[mask0])) if mask0.sum() else 0.0\n",
    "    ll1 = -np.mean(np.log(y_pred[mask1])) if mask1.sum() else 0.0\n",
    "    return 0.5 * ll0 + 0.5 * ll1\n",
    "\n",
    "\n",
    "def comp_score(y_true, y_pred):\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    wll = weighted_logloss(y_true, y_pred)\n",
    "    score = 0.5 * ap + 0.5 * (1.0 / (1.0 + wll))\n",
    "    return score, ap, wll\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4c8b9cac-8d75-4045-8ba5-39b2dbc82152",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Train shape=(10704179, 119), Test shape=(1527298, 119)\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Data loading\n",
    "# ----------------------\n",
    "train = pd.read_parquet(CFG['TRAIN_PATH'])\n",
    "_test = pd.read_parquet(CFG['TEST_PATH'])\n",
    "print(f\"[info] Train shape={train.shape}, Test shape={_test.shape}\")\n",
    "assert CFG['TARGET_COL'] in train.columns, f\"Missing target {CFG['TARGET_COL']}\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1f8480a8-3dad-4045-8928-56519a4eee20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] Numeric=111 | Categorical=6\n"
     ]
    }
   ],
   "source": [
    "# Keep ID\n",
    "if CFG['ID_COL'] in _test.columns:\n",
    "    test_id = _test[CFG['ID_COL']].astype(str).copy()\n",
    "else:\n",
    "    test_id = pd.Series([f\"TEST_{i:07d}\" for i in range(len(_test))], name='ID')\n",
    "\n",
    "# Detect numeric columns (everything except excluded and explicit categoricals)\n",
    "EXCLUDE = set([CFG['TARGET_COL'], CFG['SEQ_COL'], CFG['ID_COL']])\n",
    "feature_cols = [c for c in train.columns if c not in EXCLUDE]\n",
    "num_cols = [c for c in feature_cols if c not in CFG['CAT_COLS']]\n",
    "cat_cols = CFG['CAT_COLS']\n",
    "print(f\"[info] Numeric={len(num_cols)} | Categorical={len(cat_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ffe8e867-3cd2-487a-b6c0-2f1f84978d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] meta feature added: pred_xgb\n",
      "[info] meta feature added: pred_lgb\n"
     ]
    }
   ],
   "source": [
    "# Optional: stack v4 meta predictions if present\n",
    "meta_cols = []\n",
    "for path, name in [(CFG['V4_XGB_SUB'],'pred_xgb'), (CFG['V4_LGB_SUB'],'pred_lgb')]:\n",
    "    if Path(path).exists():\n",
    "        sub = pd.read_csv(path)\n",
    "        assert (sub['ID'].astype(str).values == test_id.astype(str).values).all(), \"ID mismatch between v5 test and v4 submission\"\n",
    "        _test[name] = sub['clicked'].astype('float32').values\n",
    "        # For train, we don't have oof here; set to NaN then fill with column mean after split (to avoid leakage)\n",
    "        train[name] = np.nan\n",
    "        meta_cols.append(name)\n",
    "        print(f\"[info] meta feature added: {name}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4b9a9c70-1274-4967-9ee2-554afd2fe5b4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[enc] gender: 3 classes\n",
      "[enc] age_group: 9 classes\n",
      "[enc] inventory_id: 18 classes\n",
      "[enc] l_feat_14: 3286 classes\n",
      "[enc] hour: 24 classes\n",
      "[enc] day_of_week: 7 classes\n"
     ]
    }
   ],
   "source": [
    "# Label encode categoricals on train+test jointly (no target leakage)\n",
    "encoders = {}\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    all_vals = pd.concat([train[col], _test[col]], axis=0).astype(str).fillna('UNK')\n",
    "    le.fit(all_vals)\n",
    "    train[col] = le.transform(train[col].astype(str).fillna('UNK'))\n",
    "    _test[col]  = le.transform(_test[col].astype(str).fillna('UNK'))\n",
    "    encoders[col] = le\n",
    "    print(f\"[enc] {col}: {len(le.classes_)} classes\")\n",
    "\n",
    "# Basic fills / types\n",
    "for c in num_cols:\n",
    "    if train[c].dtype == 'float64':\n",
    "        train[c] = train[c].astype('float32')\n",
    "        _test[c]  = _test[c].astype('float32')\n",
    "    if str(train[c].dtype).startswith('int'):\n",
    "        train[c] = train[c].astype('int32')\n",
    "        _test[c]  = _test[c].astype('int32')\n",
    "train[num_cols] = train[num_cols].fillna(0)\n",
    "_test[num_cols]  = _test[num_cols].fillna(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa289261-2f29-43b4-b674-af103ae2ceb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[split] train=9,633,761 | valid=1,070,418\n"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Validation split\n",
    "# ----------------------\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=CFG['VAL_SIZE'], random_state=CFG['SEED'])\n",
    "idx_train, idx_val = next(sss.split(train, train[CFG['TARGET_COL']]))\n",
    "tr_df = train.iloc[idx_train].reset_index(drop=True)\n",
    "va_df = train.iloc[idx_val].reset_index(drop=True)\n",
    "print(f\"[split] train={len(tr_df):,} | valid={len(va_df):,}\")\n",
    "\n",
    "# If meta features exist, fill their train part with validation-wise mean (avoid leakage)\n",
    "if meta_cols:\n",
    "    for m in meta_cols:\n",
    "        # naive backfill: use validation mean of test preds (placeholder; real oof stacking would be better)\n",
    "        tr_df[m] = tr_df[m].fillna(va_df[m].mean() if not va_df[m].isna().all() else 0.5).astype('float32')\n",
    "        va_df[m] = va_df[m].fillna(va_df[m].mean() if not va_df[m].isna().all() else 0.5).astype('float32')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "861a59e6-829f-4dd8-9ea1-5b3cb189a8ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Dataset / Dataloader\n",
    "# ----------------------\n",
    "class ClickDataset(Dataset):\n",
    "    def __init__(self, df, num_cols, cat_cols, seq_col, target_col=None, has_target=True, max_seq_len=180):\n",
    "        self.df = df.reset_index(drop=True)\n",
    "        self.num_cols = num_cols\n",
    "        self.cat_cols = cat_cols\n",
    "        self.seq_col  = seq_col\n",
    "        self.target_col = target_col\n",
    "        self.has_target = has_target\n",
    "        self.max_len = max_seq_len\n",
    "\n",
    "        self.num_X = self.df[self.num_cols].astype(np.float32).values if len(self.num_cols) else None\n",
    "        self.cat_X = self.df[self.cat_cols].astype(np.int64).values if len(self.cat_cols) else None\n",
    "        self.seq_strings = self.df[self.seq_col].astype(str).values if self.seq_col in self.df.columns else np.array(['0']*len(self.df))\n",
    "        if self.has_target:\n",
    "            self.y = self.df[self.target_col].astype(np.float32).values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        num_x = torch.tensor(self.num_X[idx], dtype=torch.float32) if self.num_X is not None else torch.empty(0)\n",
    "        cat_x = torch.tensor(self.cat_X[idx], dtype=torch.long) if self.cat_X is not None else torch.empty(0, dtype=torch.long)\n",
    "\n",
    "        # seq parse (comma-separated floats)\n",
    "        s = self.seq_strings[idx]\n",
    "        arr = np.fromstring(s, sep=',', dtype=np.float32)\n",
    "        if arr.size == 0:\n",
    "            arr = np.zeros(1, dtype=np.float32)\n",
    "        # truncate or pad to max_len handled in collate\n",
    "        seq = torch.from_numpy(arr)\n",
    "\n",
    "        if self.has_target:\n",
    "            y = torch.tensor(self.y[idx], dtype=torch.float32)\n",
    "            return num_x, cat_x, seq, y\n",
    "        else:\n",
    "            return num_x, cat_x, seq\n",
    "\n",
    "\n",
    "def collate_fn_train(batch):\n",
    "    num_x, cat_x, seqs, ys = zip(*batch)\n",
    "    num_x = torch.stack(num_x) if num_x[0].numel() else torch.zeros((len(batch),0))\n",
    "    cat_x = torch.stack(cat_x) if cat_x[0].numel() else torch.zeros((len(batch),0), dtype=torch.long)\n",
    "    ys = torch.stack(ys)\n",
    "    # pad/truncate\n",
    "    seqs = [s[:CFG['MAX_SEQ_LEN']] for s in seqs]\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    lens = torch.tensor([min(len(s), CFG['MAX_SEQ_LEN']) for s in seqs], dtype=torch.long)\n",
    "    lens = torch.clamp(lens, min=1)\n",
    "    seqs_padded = seqs_padded.unsqueeze(-1)  # (B, T, 1)\n",
    "    return num_x, cat_x, seqs_padded, lens, ys\n",
    "\n",
    "\n",
    "def collate_fn_infer(batch):\n",
    "    num_x, cat_x, seqs = zip(*batch)\n",
    "    num_x = torch.stack(num_x) if num_x[0].numel() else torch.zeros((len(batch),0))\n",
    "    cat_x = torch.stack(cat_x) if cat_x[0].numel() else torch.zeros((len(batch),0), dtype=torch.long)\n",
    "    seqs = [s[:CFG['MAX_SEQ_LEN']] for s in seqs]\n",
    "    seqs_padded = nn.utils.rnn.pad_sequence(seqs, batch_first=True, padding_value=0.0)\n",
    "    lens = torch.tensor([min(len(s), CFG['MAX_SEQ_LEN']) for s in seqs], dtype=torch.long)\n",
    "    lens = torch.clamp(lens, min=1)\n",
    "    seqs_padded = seqs_padded.unsqueeze(-1)  # (B, T, 1)\n",
    "    return num_x, cat_x, seqs_padded, lens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9af73331-fa30-44ac-bf0f-59deebd52efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Model\n",
    "# ----------------------\n",
    "class CrossNetwork(nn.Module):\n",
    "    def __init__(self, in_dim: int, num_layers: int = 2):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([nn.Linear(in_dim, 1) for _ in range(num_layers)])\n",
    "\n",
    "    def forward(self, x0):\n",
    "        x = x0\n",
    "        for layer in self.layers:\n",
    "            x = x + x0 * layer(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WideDeepCTR(nn.Module):\n",
    "    def __init__(self, num_features: int, cat_cardinalities: list, emb_dim: int = 24,\n",
    "                 rnn_type: str = 'gru', rnn_hidden: int = 96, rnn_layers: int = 2, rnn_bidir: bool = True,\n",
    "                 cross_layers: int = 2, mlp_units=(512,256,128), dropouts=(0.1,0.15,0.2)):\n",
    "        super().__init__()\n",
    "        self.num_features = num_features\n",
    "        self.cat_card = cat_cardinalities\n",
    "\n",
    "        # Embedding for categoricals\n",
    "        self.emb_layers = nn.ModuleList([\n",
    "            nn.Embedding(card, emb_dim) for card in self.cat_card\n",
    "        ])\n",
    "        cat_out = emb_dim * len(self.cat_card)\n",
    "\n",
    "        # Numeric block\n",
    "        self.bn_num = nn.BatchNorm1d(num_features) if num_features > 0 else None\n",
    "\n",
    "        # Sequence encoder\n",
    "        rnn_cls = nn.GRU if rnn_type.lower() == 'gru' else nn.LSTM\n",
    "        self.rnn = rnn_cls(input_size=1, hidden_size=rnn_hidden, num_layers=rnn_layers,\n",
    "                           batch_first=True, bidirectional=rnn_bidir)\n",
    "        seq_out = rnn_hidden * (2 if rnn_bidir else 1)\n",
    "\n",
    "        # Concatenate all\n",
    "        fused_in = (num_features if num_features>0 else 0) + cat_out + seq_out\n",
    "\n",
    "        # CrossNet\n",
    "        self.cross = CrossNetwork(fused_in, num_layers=cross_layers)\n",
    "\n",
    "        # MLP\n",
    "        layers = []\n",
    "        in_dim = fused_in\n",
    "        for i, h in enumerate(mlp_units):\n",
    "            p = dropouts[min(i, len(dropouts)-1)]\n",
    "            layers += [nn.Linear(in_dim, h), nn.ReLU(), nn.Dropout(p)]\n",
    "            in_dim = h\n",
    "        layers += [nn.Linear(in_dim, 1)]\n",
    "        self.mlp = nn.Sequential(*layers)\n",
    "\n",
    "    def forward(self, num_x, cat_x, seqs, lens):\n",
    "        parts = []\n",
    "        if self.num_features > 0:\n",
    "            x_num = self.bn_num(num_x) if self.bn_num is not None else num_x\n",
    "            parts.append(x_num)\n",
    "        if len(self.cat_card) > 0:\n",
    "            embs = [emb(cat_x[:, i]) for i, emb in enumerate(self.emb_layers)]\n",
    "            parts.append(torch.cat(embs, dim=1))\n",
    "        # Sequence\n",
    "        packed = nn.utils.rnn.pack_padded_sequence(seqs, lens.cpu(), batch_first=True, enforce_sorted=False)\n",
    "        if isinstance(self.rnn, nn.LSTM):\n",
    "            _, (h_n, _) = self.rnn(packed)\n",
    "        else:\n",
    "            _, h_n = self.rnn(packed)\n",
    "        if self.rnn.bidirectional:\n",
    "            h = torch.cat([h_n[-2], h_n[-1]], dim=1)\n",
    "        else:\n",
    "            h = h_n[-1]\n",
    "        parts.append(h)\n",
    "\n",
    "        z = torch.cat(parts, dim=1)\n",
    "        z_cross = self.cross(z)\n",
    "        logits = self.mlp(z_cross).squeeze(1)\n",
    "        return logits\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2d184d0-933d-4e9d-b147-5e3adb91d221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[info] pos_ratio=0.019075, pos_weight=51.43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------\n",
    "# Build loaders\n",
    "# ----------------------\n",
    "ALL_NUM = num_cols.copy()\n",
    "ALL_CAT = cat_cols.copy()\n",
    "\n",
    "train_ds = ClickDataset(tr_df, ALL_NUM, ALL_CAT, CFG['SEQ_COL'], CFG['TARGET_COL'], True, CFG['MAX_SEQ_LEN'])\n",
    "valid_ds = ClickDataset(va_df, ALL_NUM, ALL_CAT, CFG['SEQ_COL'], CFG['TARGET_COL'], True, CFG['MAX_SEQ_LEN'])\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=CFG['BATCH_SIZE'], shuffle=True,\n",
    "                          num_workers=CFG['NUM_WORKERS'], pin_memory=CFG['PIN_MEMORY'], collate_fn=collate_fn_train)\n",
    "valid_loader = DataLoader(valid_ds, batch_size=CFG['BATCH_SIZE'], shuffle=False,\n",
    "                          num_workers=CFG['NUM_WORKERS'], pin_memory=CFG['PIN_MEMORY'], collate_fn=collate_fn_train)\n",
    "\n",
    "# ----------------------\n",
    "# Model / Optimizer / Loss\n",
    "# ----------------------\n",
    "cat_cardinalities = [int(train[c].max()) + 1 for c in ALL_CAT]\n",
    "model = WideDeepCTR(num_features=len(ALL_NUM), cat_cardinalities=cat_cardinalities,\n",
    "                    emb_dim=CFG['EMB_DIM'], rnn_type=CFG['RNN_TYPE'], rnn_hidden=CFG['RNN_HIDDEN'],\n",
    "                    rnn_layers=CFG['RNN_LAYERS'], rnn_bidir=CFG['RNN_BIDIR'], cross_layers=CFG['CROSS_LAYERS'],\n",
    "                    mlp_units=CFG['MLP_UNITS'], dropouts=CFG['DROPOUTS']).to(device)\n",
    "\n",
    "# pos_weight\n",
    "pos_ratio = tr_df[CFG['TARGET_COL']].mean()\n",
    "neg = len(tr_df) - tr_df[CFG['TARGET_COL']].sum()\n",
    "pos = tr_df[CFG['TARGET_COL']].sum()\n",
    "pos_weight_val = (neg / max(pos,1)) if pos > 0 else 1.0\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([pos_weight_val], dtype=torch.float, device=device))\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=CFG['LR'], weight_decay=CFG['WEIGHT_DECAY'])\n",
    "scheduler = torch.optim.lr_scheduler.CosineAnnealingWarmRestarts(optimizer, T_0=2, T_mult=2)\n",
    "\n",
    "print(f\"[info] pos_ratio={pos_ratio:.6f}, pos_weight={pos_weight_val:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ea8b98-6576-4bd4-a39e-68abc4c6b0e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Train 1]:   4%|████▊                                                                                                             | 392/9408 [5:55:58<138:05:02, 55.14s/it]"
     ]
    }
   ],
   "source": [
    "# ----------------------\n",
    "# Train\n",
    "# ----------------------\n",
    "BEST = {'score': -1, 'epoch': -1}\n",
    "logits_val_cache = None\n",
    "\n",
    "for epoch in range(1, CFG['EPOCHS']+1):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for num_x, cat_x, seqs, lens, ys in tqdm(train_loader, desc=f\"[Train {epoch}]\"):\n",
    "        num_x, cat_x, seqs, lens, ys = num_x.to(device), cat_x.to(device), seqs.to(device), lens.to(device), ys.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        logits = model(num_x, cat_x, seqs, lens)\n",
    "        loss = criterion(logits, ys)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step(epoch + len(train_loader))\n",
    "        running_loss += loss.item() * ys.size(0)\n",
    "    train_loss = running_loss / len(train_ds)\n",
    "\n",
    "    # validate\n",
    "    model.eval()\n",
    "    val_logits, val_targets = [], []\n",
    "    with torch.no_grad():\n",
    "        for num_x, cat_x, seqs, lens, ys in tqdm(valid_loader, desc=f\"[Valid {epoch}]\"):\n",
    "            num_x, cat_x, seqs, lens = num_x.to(device), cat_x.to(device), seqs.to(device), lens.to(device)\n",
    "            logits = model(num_x, cat_x, seqs, lens)\n",
    "            val_logits.append(logits.detach().cpu())\n",
    "            val_targets.append(ys)\n",
    "    val_logits = torch.cat(val_logits).numpy()\n",
    "    val_targets = torch.cat(val_targets).numpy()\n",
    "    val_probs = 1 / (1 + np.exp(-val_logits))\n",
    "\n",
    "    score, ap, wll = comp_score(val_targets, val_probs)\n",
    "    print(f\"[Epoch {epoch}] train_loss={train_loss:.5f} | score={score:.6f} | AP={ap:.6f} | WLL={wll:.6f}\")\n",
    "\n",
    "    if score > BEST['score']:\n",
    "        BEST.update({'score': score, 'epoch': epoch})\n",
    "        logits_val_cache = val_logits.copy()\n",
    "    gc.collect()\n",
    "\n",
    "print(f\"[best] epoch={BEST['epoch']} | score={BEST['score']:.6f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3481d436-8984-4668-9bdd-1e90b010c847",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ----------------------\n",
    "# Platt scaling (calibration) on validation\n",
    "# ----------------------\n",
    "calibrated = False\n",
    "if logits_val_cache is not None:\n",
    "    try:\n",
    "        lr_cal = LogisticRegression(max_iter=1000)\n",
    "        lr_cal.fit(logits_val_cache.reshape(-1,1), (val_targets>0.5).astype(int))\n",
    "        calibrated = True\n",
    "        print(\"[calib] Platt scaling trained.\")\n",
    "    except Exception as e:\n",
    "        print(\"[calib] Failed:\", e)\n",
    "\n",
    "# ----------------------\n",
    "# Inference on test\n",
    "# ----------------------\n",
    "# Rebuild full train loader for final fit if desired (optional). Here we reuse current weights.\n",
    "# Create test loader\n",
    "\n",
    "test_ds = ClickDataset(_test, ALL_NUM, ALL_CAT, CFG['SEQ_COL'], has_target=False, max_seq_len=CFG['MAX_SEQ_LEN'])\n",
    "test_loader = DataLoader(test_ds, batch_size=CFG['BATCH_SIZE'], shuffle=False,\n",
    "                         num_workers=CFG['NUM_WORKERS'], pin_memory=CFG['PIN_MEMORY'], collate_fn=collate_fn_infer)\n",
    "\n",
    "model.eval()\n",
    "all_outs = []\n",
    "with torch.no_grad():\n",
    "    for num_x, cat_x, seqs, lens in tqdm(test_loader, desc='[Inference]'):\n",
    "        num_x, cat_x, seqs, lens = num_x.to(device), cat_x.to(device), seqs.to(device), lens.to(device)\n",
    "        logits = model(num_x, cat_x, seqs, lens)\n",
    "        all_outs.append(logits.detach().cpu())\n",
    "\n",
    "logits_test = torch.cat(all_outs).numpy()\n",
    "probs_test = 1 / (1 + np.exp(-logits_test))\n",
    "\n",
    "if calibrated:\n",
    "    probs_test = lr_cal.predict_proba(logits_test.reshape(-1,1))[:,1]\n",
    "\n",
    "# clip safety\n",
    "probs_test = np.clip(probs_test, 0.0, 1.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54016ac9-10a6-40b7-8fc5-840492d7325e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build submission\n",
    "submit = pd.DataFrame({\n",
    "    'ID': test_id.values.astype(str),\n",
    "    'clicked': probs_test.astype(np.float32)\n",
    "})\n",
    "\n",
    "submit.to_csv(CFG['SUBMIT_PATH'], index=False)\n",
    "print(f\"[save] submission -> {CFG['SUBMIT_PATH']}\")\n",
    "print(\"[check] clicked range: %.6f ~ %.6f\" % (submit['clicked'].min(), submit['clicked'].max()))\n",
    "\n",
    "print(\"[done] v5 Neural Hybrid pipeline complete.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "ml_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
