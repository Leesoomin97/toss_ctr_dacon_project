{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4c6a2c43-a855-466b-a377-cd1c0b082bdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ===============================================\n",
    "# Toss CTR — DCN (Split, Fast & Safe)\n",
    "# ===============================================\n",
    "import os, gc, math, random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "from sklearn.metrics import average_precision_score, log_loss\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d6efdc4d-3b70-4dc1-84fd-8040c8dc83a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/envs/ml_project/lib/python3.11/site-packages/torch/cuda/__init__.py:129: UserWarning: CUDA initialization: CUDA driver initialization failed, you might not have a CUDA gpu. (Triggered internally at ../c10/cuda/CUDAFunctions.cpp:108.)\n",
      "  return torch._C._cuda_getDeviceCount() > 0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# 0) Repro & Device\n",
    "SEED = 42\n",
    "def set_seed(s=SEED):\n",
    "    random.seed(s); np.random.seed(s); torch.manual_seed(s); torch.cuda.manual_seed_all(s)\n",
    "    torch.backends.cudnn.deterministic = True; torch.backends.cudnn.benchmark = False\n",
    "set_seed()\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n",
    "\n",
    "# 1) Load\n",
    "train = pd.read_parquet(\"train_input_2.parquet\")\n",
    "test  = pd.read_parquet(\"test_input_2.parquet\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4f0ec546-712e-4229-a93e-046aa87ee2e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features: 26 | Pos ratio: 0.0191\n"
     ]
    }
   ],
   "source": [
    "# 2) Columns\n",
    "id_cols = [c for c in [\"row_id\",\"id\"] if c in train.columns or c in test.columns]\n",
    "id_cols_train = [c for c in id_cols if c in train.columns]\n",
    "id_cols_test  = [c for c in id_cols if c in test.columns]\n",
    "target_col = \"clicked\"\n",
    "assert target_col in train.columns, \"clicked target missing in train_input_2.parquet\"\n",
    "\n",
    "# 3) Feature matrix (handle stray object dtypes)\n",
    "X = train.drop(columns=id_cols_train + [target_col]).copy()\n",
    "y = train[target_col].astype(np.float32).values\n",
    "X_test = test.drop(columns=id_cols_test).copy()\n",
    "\n",
    "# Convert any object columns to numeric via shared LabelEncoder\n",
    "for c in X.columns:\n",
    "    if X[c].dtype == \"object\" or X_test[c].dtype == \"object\":\n",
    "        le = LabelEncoder()\n",
    "        both = pd.concat([X[c].astype(str), X_test[c].astype(str)], axis=0)\n",
    "        le.fit(both)\n",
    "        X[c] = le.transform(X[c].astype(str))\n",
    "        X_test[c] = le.transform(X_test[c].astype(str))\n",
    "\n",
    "# Ensure float32 tensors\n",
    "X = X.astype(np.float32)\n",
    "X_test = X_test.astype(np.float32)\n",
    "\n",
    "X = X.clip(-3, 3).fillna(0)\n",
    "X_test = X_test.clip(-3, 3).fillna(0)\n",
    "\n",
    "FEATURES = X.columns.tolist()\n",
    "print(f\"Features: {len(FEATURES)} | Pos ratio: {y.mean():.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "761df8ff-232e-447b-ab3b-768cf5c0c3ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4) Split (80/20 stratified)\n",
    "sss = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=SEED)\n",
    "train_idx, val_idx = next(sss.split(X, y))\n",
    "X_tr, X_va = X.iloc[train_idx].values, X.iloc[val_idx].values\n",
    "y_tr, y_va = y[train_idx], y[val_idx]\n",
    "\n",
    "# 5) Datasets\n",
    "class CTRDataset(Dataset):\n",
    "    def __init__(self, X, y=None):\n",
    "        self.X = X\n",
    "        self.y = y\n",
    "    def __len__(self): return len(self.X)\n",
    "    def __getitem__(self, i):\n",
    "        x = torch.from_numpy(self.X[i])\n",
    "        if self.y is None: return x\n",
    "        return x, torch.tensor(self.y[i], dtype=torch.float32)\n",
    "\n",
    "BATCH = 2048\n",
    "train_ds = CTRDataset(X_tr, y_tr)\n",
    "val_ds   = CTRDataset(X_va, y_va)\n",
    "test_ds  = CTRDataset(X_test.values)\n",
    "\n",
    "train_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True, num_workers=2, pin_memory=True)\n",
    "val_loader   = DataLoader(val_ds, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
    "test_loader  = DataLoader(test_ds, batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c2826a56-b1c8-45a8-a8d5-ea7f6df8ab64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6) DCN Model (Cross Network + Deep MLP)\n",
    "class CrossLayer(nn.Module):\n",
    "    def __init__(self, dim):\n",
    "        super().__init__()\n",
    "        self.w = nn.Parameter(torch.randn(dim))\n",
    "        self.b = nn.Parameter(torch.zeros(dim))\n",
    "    def forward(self, x0, x):\n",
    "        # x_{l+1} = x0 * (w^T x) + b + x\n",
    "        # (batch, dim)\n",
    "        dot = (x @ self.w)  # (batch,)\n",
    "        dot = torch.clamp(dot, -3, 3)\n",
    "        return x0 * dot.unsqueeze(1) + self.b + x\n",
    "\n",
    "class DCN(nn.Module):\n",
    "    def __init__(self, dim, cross_layers=3, hidden=(512,256,64), p_drop=0.5):\n",
    "        super().__init__()\n",
    "        self.cross = nn.ModuleList([CrossLayer(dim) for _ in range(cross_layers)])\n",
    "        layers = []\n",
    "        in_dim = dim\n",
    "        for h in hidden:\n",
    "            layers += [nn.Linear(in_dim, h), nn.ReLU(inplace=True), nn.Dropout(p_drop)]\n",
    "            in_dim = h\n",
    "        self.deep = nn.Sequential(*layers)\n",
    "        self.out = nn.Linear(in_dim + dim, 1)  # concatenate cross_out & deep_out\n",
    "    def forward(self, x):\n",
    "        x0 = x\n",
    "        xc = x\n",
    "        for cl in self.cross:\n",
    "            xc = cl(x0, xc)\n",
    "        xd = self.deep(x)\n",
    "        z = torch.cat([xc, xd], dim=1)\n",
    "        return self.out(z).squeeze(1)  # logits\n",
    "\n",
    "in_dim = len(FEATURES)\n",
    "model = DCN(dim=in_dim, cross_layers=3, hidden=(512,256,64), p_drop=0.3).to(device)\n",
    "\n",
    "# 7) Loss & Optim\n",
    "pos = y_tr.sum(); neg = len(y_tr) - pos\n",
    "pos_weight = torch.tensor((neg / max(1.0, pos)), dtype=torch.float32, device=device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)  # handles imbalance\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-5)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4fd21ae2-1029-4be6-a463-f9fc5a0b6717",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Epoch 1] train_loss=1.243238 | val_AP=0.061478 | val_WLL=0.610103\n",
      "[Epoch 2] train_loss=1.221125 | val_AP=0.061988 | val_WLL=0.639057\n",
      "[Epoch 3] train_loss=1.217878 | val_AP=0.063414 | val_WLL=0.654988\n",
      "[Epoch 4] train_loss=1.216086 | val_AP=0.063907 | val_WLL=0.641378\n",
      "[Epoch 5] train_loss=1.214915 | val_AP=0.063307 | val_WLL=0.616376\n",
      "[Epoch 6] train_loss=1.213401 | val_AP=0.064629 | val_WLL=0.592755\n",
      "[Epoch 7] train_loss=1.212522 | val_AP=0.064181 | val_WLL=0.620692\n",
      "[Epoch 8] train_loss=1.211957 | val_AP=0.064743 | val_WLL=0.630372\n",
      "[Epoch 9] train_loss=1.211121 | val_AP=0.064231 | val_WLL=0.595105\n",
      "[Epoch 10] train_loss=1.210466 | val_AP=0.064632 | val_WLL=0.638846\n",
      "[Epoch 11] train_loss=1.209793 | val_AP=0.064929 | val_WLL=0.605283\n",
      "[Epoch 12] train_loss=1.209587 | val_AP=0.065267 | val_WLL=0.636680\n"
     ]
    }
   ],
   "source": [
    "# 8) Train (Early Stopping on val AUPRC)\n",
    "EPOCHS = 12\n",
    "PATIENCE = 3\n",
    "best_ap = -1.0\n",
    "best_state = None\n",
    "no_improve = 0\n",
    "\n",
    "def eval_loop(loader):\n",
    "    model.eval()\n",
    "    ys, ps = [], []\n",
    "    with torch.no_grad():\n",
    "        for batch in loader:\n",
    "            xb, yb = batch\n",
    "            xb = xb.to(device)\n",
    "            logits = model(xb)\n",
    "            prob = torch.sigmoid(logits).float().cpu().numpy()\n",
    "            # --- 여기 추가 ---\n",
    "            prob = np.nan_to_num(prob, nan=0.5, posinf=1.0, neginf=0.0)\n",
    "            # ----------------\n",
    "            ps.append(prob)\n",
    "            ys.append(yb.numpy())\n",
    "    y_true = np.concatenate(ys)\n",
    "    y_pred = np.concatenate(ps)\n",
    "    ap = average_precision_score(y_true, y_pred)\n",
    "    wll = log_loss(y_true, np.clip(y_pred, 1e-6, 1-1e-6))\n",
    "    return ap, wll\n",
    "\n",
    "for epoch in range(1, EPOCHS+1):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    for batch in train_loader:\n",
    "        xb, yb = batch\n",
    "        xb = xb.to(device, non_blocking=True)\n",
    "        yb = yb.to(device, non_blocking=True)\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "        logits = model(xb)\n",
    "        loss = criterion(logits, yb)\n",
    "        if torch.isnan(loss):\n",
    "         print(\"[WARN] NaN loss detected — skipping batch\")\n",
    "         continue\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item() * xb.size(0)\n",
    "\n",
    "    ap, wll = eval_loop(val_loader)\n",
    "    print(f\"[Epoch {epoch}] train_loss={total_loss/len(train_ds):.6f} | val_AP={ap:.6f} | val_WLL={wll:.6f}\")\n",
    "\n",
    "    if ap > best_ap + 1e-5:\n",
    "        best_ap = ap\n",
    "        best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "        no_improve = 0\n",
    "    else:\n",
    "        no_improve += 1\n",
    "        if no_improve >= PATIENCE:\n",
    "            print(f\"Early stopping at epoch {epoch} (best AP={best_ap:.6f})\")\n",
    "            break\n",
    "\n",
    "# load best\n",
    "if best_state is not None:\n",
    "    model.load_state_dict(best_state)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "34ae0e99-a327-4811-8dc0-c6f31f1a864f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Final] val_AP=0.065267 | val_WLL=0.636680\n",
      "[Saved] Validation preds → dcn_val_pred.npy (len=2140834)\n",
      "[Saved] Final submission → toss_dcn_v8_submit.csv\n"
     ]
    }
   ],
   "source": [
    "# ===============================================\n",
    "# 9️⃣ Predict Test & Save Validation for Blending\n",
    "# ===============================================\n",
    "# ===============================================\n",
    "# 9️⃣ Predict Test & Save Validation for Blending\n",
    "# ===============================================\n",
    "model.eval()\n",
    "\n",
    "# --- Predict test ---\n",
    "all_probs = []\n",
    "with torch.no_grad():\n",
    "    for xb in test_loader:\n",
    "        xb = xb.to(device)\n",
    "        prob = torch.sigmoid(model(xb)).float().cpu().numpy()\n",
    "        all_probs.append(prob)\n",
    "test_pred = np.concatenate(all_probs)\n",
    "test_pred = np.clip(test_pred, 1e-4, 1-1e-4)\n",
    "\n",
    "# --- Evaluate on validation ---\n",
    "ap, wll = eval_loop(val_loader)\n",
    "print(f\"[Final] val_AP={ap:.6f} | val_WLL={wll:.6f}\")\n",
    "\n",
    "# --- Save validation info for blending ---\n",
    "np.save(\"dcn_val_index.npy\", val_idx)\n",
    "\n",
    "val_probs = []\n",
    "with torch.no_grad():\n",
    "    for xb, _ in val_loader:\n",
    "        xb = xb.to(device)\n",
    "        val_probs.append(torch.sigmoid(model(xb)).cpu().numpy())\n",
    "val_pred = np.concatenate(val_probs).ravel()\n",
    "\n",
    "np.save(\"dcn_val_pred.npy\", val_pred)\n",
    "np.save(\"dcn_val_true.npy\", y_va)\n",
    "\n",
    "print(f\"[Saved] Validation preds → dcn_val_pred.npy (len={len(val_pred)})\")\n",
    "\n",
    "# --- ✅ Create final submission file ---\n",
    "id_col = \"id\" if \"id\" in test.columns else \"row_id\"\n",
    "\n",
    "submit = pd.DataFrame({\n",
    "    \"ID\": test[id_col],       # ← 여기서 무조건 'ID'로 강제 변경\n",
    "    \"clicked\": test_pred\n",
    "})\n",
    "submit.to_csv(\"toss_dcn_v8_submit.csv\", index=False)\n",
    "\n",
    "print(\"[Saved] Final submission → toss_dcn_v8_submit.csv\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ed67c1e-a3d2-4cbc-9f77-707861898eb0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ml_project",
   "language": "python",
   "name": "ml_project"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
